{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fifa Position Predictor\n",
    "\n",
    "- Ali Elmancy 201901993\n",
    "- Ahmed Abdou 202005605\n",
    "- Ahmed Terchoun 201802794"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Cleaning\n",
    "\n",
    "At this stage of the project we began by dividing the attributes under two categories, main and detailed, according to our knowledge of the game. The main attributes represent player stats present on his card while detailed ones are internal and usually reflect the main stats but contain extra attributes. We did neglect some other columns that are irrelevant to the task such as the player club and shirt number in order to reduce the dimensionality and provide the model accurate data. After doing so we ended up with a total of 42 columns. \n",
    "\n",
    "Furthermore, we dropped all goalkeepers as their stats are totally different from other positions and they can be easily distinguished. Also, the column mentality_composure was dropped due to having almost 18k nan values.\n",
    "\n",
    "After that, we went to process our target column which is player_positions, it included a list of positions in which a player can play in. However, we created our own encoding class so that we can map player positions into one of seven main categories [ST, LW, RW, CM, LB, RB, CB]. We chose the player's first position to be mapped as it is the position at which the player performs the best. Then we renamed this column to positions and added it to the end of the dataframe.\n",
    "\n",
    "After this step we saved a copy of this dataframe called cleaned_df which is later used for EDA. Moving forward we had two categorical columns, preferred_foot and work_rate, that had text values, so we used one-hot encoding technique to transform these columns so that they can be used for neural networks. The main reason behind choosing one-hot encoding over other encoding techniques is that it does not assign any priorty to the values unlike label encoding. That is why it worked best for our dataset.\n",
    "\n",
    "Moreover, we used our encoding function from the coder class to encode the players' positions to numerical values from 0 to 6, and saved this version of the dataframe as encoded_df which is going to be used later for PCA.\n",
    "\n",
    "Finally, we proceeded by normalizing all the continuous values columns in order to prepare the data for training. We saved this version of the dataframe as normalized_df. The final shape of the dataframe after dropping the unnecessary columns and encoding the categorical ones is 143613 rows and 50 columns. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. EDA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Feature Selection\n",
    "\n",
    "In this part of the project we tried to filter out the unnecessary features and pick up the ones that contribute the most to the output of the classifier.\n",
    "\n",
    "We began by dividing the features of our dataset into seven categories:\n",
    "\n",
    "- MAIN_FIFA_CARD_FEAURES\n",
    "- ATTACKING_FEATURES\n",
    "- SKILL_FEATURES\n",
    "- MOVE_FEATURES\n",
    "- POWER_FEATURES\n",
    "- MENTAL_FEATURES\n",
    "- DEFENDING_FEATURES\n",
    "\n",
    "Based on the results we got from EDA, we combined weight and height into one column by taking the sum and dividing by 2. We then iterated through the main_features and every other feature to capture the correlation among them. Any feature that had a correlation greater than |0.8| with one of the main features was added to a list of columns to be dropped. The reasoning behind this step is that most of the internal features appeared to have high correlation with the main stats of the player from the done EDA, and removing them would reduce the redundancy of information.\n",
    "\n",
    "After removing these features, we wanted to figure out which columns have weak correlation with our target. Therefore, we iterated over all the columns and dropped any column that has a correlation less than or equal to |0.1| with the position column. By this step, we concluded the manual feature selection.\n",
    "\n",
    "As we wanted to further experiment with feature selection, we used PCA to reduce the dimensionality of the dataset while keeping most of the information present. We tried different ranges of components and chose the best one, which is 12 as it keeps 80% of the dataset variance. With the new components, the difference in accuracy while testing with initial models was minimal, so we might continue working with them. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model\n",
    "At this stage we tried couple of initial models to a get a baseline performance for our final model. The initial model we decided to try was RandomForestClassifier since our task is classification. The training accuracy of the model on the final data was 0.84 and testing accuracy was 0.84 as well. However, when we used the same classifier on the pca data we got a training accuracy of 0.8 and testing accuracy of 0.796. From the previous results we can see that the performance is not that different considering that when using pca the number of features is less than 45% of the final data. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
